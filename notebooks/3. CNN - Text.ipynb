{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train convolutional network for sentiment analysis. \n",
    "\n",
    "Based on\n",
    "\"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "\n",
    "For `CNN-non-static` gets to 82.1% after 61 epochs with following settings:\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 3\n",
    "dropout_prob = (0.7, 0.8)\n",
    "hidden_dims = 100\n",
    "\n",
    "For `CNN-rand` gets to 78-79% after 7-8 epochs with following settings:\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "For `CNN-static` gets to 75.4% after 7 epochs with following settings:\n",
    "embedding_dim = 100          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "* it turns out that such a small data set as \"Movie reviews with one\n",
    "sentence per review\"  (Pang and Lee, 2005) requires much smaller network\n",
    "than the one introduced in the original article:\n",
    "- embedding dimension is only 20 (instead of 300; 'CNN-static' still requires ~100)\n",
    "- 2 filter sizes (instead of 3)\n",
    "- higher dropout probabilities and\n",
    "- 3 filters per filter size is enough for 'CNN-non-static' (instead of 100)\n",
    "- embedding initialization does not require prebuilt Google Word2Vec data.\n",
    "Training Word2Vec on the same \"Movie reviews\" data set is enough to \n",
    "achieve performance reported in the article (81.6%)\n",
    "\n",
    "Another distinct difference is sliding MaxPooling window of length=2\n",
    "instead of MaxPooling over whole feature map as in the article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data_helpers\n",
    "from w2v import train_word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_variation = 'CNN-rand'  #  CNN-rand | CNN-non-static | CNN-static\n",
    "print('Model variation is %s' % model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "sequence_length = 56\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec parameters, see train_word2vec\n",
    "min_word_count = 1  # Minimum word count                        \n",
    "context = 10        # Context window size    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "x, y, vocabulary, vocabulary_inv = data_helpers.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if model_variation=='CNN-non-static' or model_variation=='CNN-static':\n",
    "    embedding_weights = train_word2vec(x, vocabulary_inv, embedding_dim, min_word_count, context)\n",
    "    if model_variation=='CNN-static':\n",
    "        x = embedding_weights[0][x]\n",
    "elif model_variation=='CNN-rand':\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError('Unknown model variation')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.append(x,y,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.15,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = test[:,:56]\n",
    "Y_test = test[:,56:58]\n",
    "\n",
    "\n",
    "X_train = train[:,:56]\n",
    "Y_train = train[:,56:58]\n",
    "train_rows = np.random.randint(0,X_train.shape[0],2500)\n",
    "X_train = X_train[train_rows]\n",
    "Y_train = Y_train[train_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    \n",
    "    global graph_in\n",
    "    global convs\n",
    "    \n",
    "    graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "    convs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Buliding the first layer (Convolution Layer) of the network\n",
    "def build_layer_1(filter_length):\n",
    "    \n",
    "   \n",
    "    conv = Convolution1D(nb_filter=num_filters,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding a max pooling layer to the model(network)\n",
    "def add_max_pooling(conv):\n",
    "    \n",
    "    pool = MaxPooling1D(pool_length=2)(conv)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding a flattening layer to the model(network), before adding a dense layer\n",
    "def add_flatten(conv_or_pool):\n",
    "    \n",
    "    flatten = Flatten()(conv_or_pool)\n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_sequential(graph):\n",
    "    \n",
    "    #main sequential model\n",
    "    model = Sequential()\n",
    "    if not model_variation=='CNN-static':\n",
    "        model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length,\n",
    "                        weights=embedding_weights))\n",
    "    model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "    model.add(graph)\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Flatten\n",
    "def one_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    flatten = add_flatten(conv)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=1, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Max Pooling 3.Flatten\n",
    "def two_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Flatten\n",
    "def three_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    conv = build_layer_1(4)\n",
    "    flatten = add_flatten(conv)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    if len(filter_sizes)>1:\n",
    "        out = Merge(mode='concat')(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=1, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Max Pooling 6.Flatten\n",
    "def four_layer_convolution():\n",
    "    \n",
    "    initialize()\n",
    "    \n",
    "    conv = build_layer_1(3)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    conv = build_layer_1(4)\n",
    "    pool = add_max_pooling(conv)\n",
    "    flatten = add_flatten(pool)\n",
    "    \n",
    "    convs.append(flatten)\n",
    "    \n",
    "    if len(filter_sizes)>1:\n",
    "        out = Merge(mode='concat')(convs)\n",
    "    else:\n",
    "        out = convs[0]\n",
    "\n",
    "    graph = Model(input=graph_in, output=out)\n",
    "    \n",
    "    model = add_sequential(graph)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Flatten\n",
    "one_layer_convolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Max Pooling 3.Flatten\n",
    "two_layer_convolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Flatten\n",
    "three_layer_convolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#1.Convolution 2.Max Pooling 3.Flatten 4.Convolution 5.Max Pooling 6.Flatten\n",
    "four_layer_convolution()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tuthpc]",
   "language": "python",
   "name": "Python [tuthpc]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
