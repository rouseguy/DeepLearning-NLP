{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A basic sequence-to-sequence model, as introduced in Cho et al., 2014 (pdf), consists of two recurrent neural networks (RNNs): an encoder that processes the input and a decoder that generates the output. \n",
    "\n",
    "Every Seq2seq model has 2 primary layers : the encoder and the decoder.  Generally, the encoder encodes the input sequence to an internal representation called 'context vector'  which is used by the decoder to generate the output sequence. \n",
    "\n",
    "The lengths of input and output sequences can be different, as there is no explicit one on one relation between the input and output sequences. \n",
    "\n",
    "#Source : https://github.com/farizrahman4u/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.engine.training import slice_arrays\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent\n",
    "import numpy as np\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, chars, maxlen):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "RNN = recurrent.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars, MAXLEN)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "CPU times: user 4.89 s, sys: 41.9 ms, total: 4.93 s\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Generating random  numbers to perofrm addition on\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that X+Y == Y+X (hence the sorting)\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "#We now have 50000 examples of addition, each exaple contains the addition between two numbers\n",
    "#Each example contains the first number followed by '+' operand followed by the second number \n",
    "#examples - 85+96, 353+551, 6+936\n",
    "#The answers to the additon operation are stored in expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Look into the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "CPU times: user 472 ms, sys: 11.3 ms, total: 483 ms\n",
      "Wall time: 481 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#The above questions and answers are going to be one hot encoded, \n",
    "#before training.\n",
    "#The encoded values will be used to train the model\n",
    "#The maximum length of a question can be 7 \n",
    "#(3 digits followed by '+' followed by 3 digits)\n",
    "#The maximum length of an answer can be 4 \n",
    "#(Since the addition of 3 digits yields either a 3 digit number or a 4\n",
    "#4 digit number)\n",
    "\n",
    "#Now for training each number or operand is going to be one hot encode below\n",
    "#In one hot encode there are 12 possibilities '0123456789+ ' (The last one is a space)\n",
    "#Since we assume a maximum of 3 digit numbers, a two digit number is taken as space with two digts, or \n",
    "#a single digit number as two spaces with a number\n",
    "\n",
    "#So for questions we get 7 rows since the max possible length is 7, and each row has a length of 12 because it will\n",
    "#be one hot encoded with True and False, depending on the character(any one of the number, '+' operand, or space)\n",
    "#will be stored  in X_train and X_val\n",
    "#The 4th position in(1,2,3,4,5,6,7) will indicate the one hot encoding of the '+' operand\n",
    "\n",
    "##So for questions we get 4 rows since the max possible length is 4, and each row has a length of 12 because it will\n",
    "#be one hot encoded with True and False, depending on the character(any one of the number, '+' operand, or space)\n",
    "#will be stored  in y_train and y_val\n",
    "\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, maxlen=DIGITS + 1)\n",
    "\n",
    "# Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over\n",
    "split_at = int(len(X) - len(X) / 10)\n",
    "(X_train, X_val) = (X[:split_at], X[split_at:])\n",
    "(y_train, y_val) = (y[:split_at], y[split_at:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_val))\n",
    "print(np.shape(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "CPU times: user 1.58 s, sys: 65.7 ms, total: 1.65 s\n",
      "Wall time: 724 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Training the model with the encoded inputs\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, nb_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "model.add(RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributed(Dense(len(chars))))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s - loss: 1.8436 - acc: 0.3339 - val_loss: 1.7053 - val_acc: 0.3648\n",
      "\n",
      "\n",
      "Test score: 1.70534285183\n",
      "Test accuracy: 0.3648\n",
      "\n",
      "\n",
      "CPU times: user 1min 21s, sys: 4.57 s, total: 1min 26s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model each generation and show predictions against the validation dataset\n",
    "for iteration in range(1, 2):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=1,\n",
    "              validation_data=(X_val, y_val))\n",
    "    \n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print('\\n')\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 19+66  \n",
      "T 85  \n",
      "\u001b[91m☒\u001b[0m 10  \n",
      "---\n",
      "Q 525+14 \n",
      "T 539 \n",
      "\u001b[91m☒\u001b[0m 556 \n",
      "---\n",
      "Q 0+784  \n",
      "T 784 \n",
      "\u001b[91m☒\u001b[0m 101 \n",
      "---\n",
      "Q 153+726\n",
      "T 879 \n",
      "\u001b[91m☒\u001b[0m 111 \n",
      "---\n",
      "Q 885+55 \n",
      "T 940 \n",
      "\u001b[91m☒\u001b[0m 109 \n",
      "---\n",
      "Q 234+129\n",
      "T 363 \n",
      "\u001b[91m☒\u001b[0m 321 \n",
      "---\n",
      "Q 276+880\n",
      "T 1156\n",
      "\u001b[91m☒\u001b[0m 1011\n",
      "---\n",
      "Q 35+35  \n",
      "T 70  \n",
      "\u001b[91m☒\u001b[0m 55  \n",
      "---\n",
      "Q 383+766\n",
      "T 1149\n",
      "\u001b[91m☒\u001b[0m 101 \n",
      "---\n",
      "Q 250+492\n",
      "T 742 \n",
      "\u001b[91m☒\u001b[0m 101 \n",
      "---\n",
      "CPU times: user 1.65 s, sys: 35.5 ms, total: 1.68 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#For predicting the outputs, the predict method will return \n",
    "#an one hot encoded ouput, we decode the one hot encoded \n",
    "#ouptut to get our final output\n",
    "\n",
    "# Select 10 samples from the validation set at random so we can visualize errors\n",
    "for i in range(10):\n",
    "    ind = np.random.randint(0, len(X_val))\n",
    "    rowX, rowy = X_val[np.array([ind])], y_val[np.array([ind])]\n",
    "    preds = model.predict_classes(rowX, verbose=0)\n",
    "    q = ctable.decode(rowX[0])\n",
    "    correct = ctable.decode(rowy[0])\n",
    "    guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "    print('Q', q[::-1] if INVERT else q)\n",
    "    print('T', correct)\n",
    "    print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tuthpc]",
   "language": "python",
   "name": "Python [tuthpc]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
